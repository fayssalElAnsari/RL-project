{"cells":[{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tqdm import tqdm\n","import gym\n","import matplotlib.pyplot as plt\n","import time"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Hyperparameters\n","n_episodes = 10000\n","batch_size = 10\n","gamma = 0.95\n","learning_rate = 0.1\n","epsilon_decay = 0.09\n","n_actions = 4  # Number of actions in FrozenLake"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["negative_reward_enabled = False\n","is_slippery_enabled = False"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["custom_map = [\n","    'SFFF',\n","    'FHFF',\n","    'FFHF',\n","    'HFGF'\n","]"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Initialize the environment\n","env = gym.make('FrozenLake-v1', desc=custom_map, is_slippery=is_slippery_enabled)\n","n_states = env.observation_space.n\n","\n","# Create the Q-network model\n","model = keras.Sequential([\n","    keras.layers.InputLayer(batch_input_shape=(1, n_states)),\n","    keras.layers.Dense(10, activation='relu'),\n","    keras.layers.Dense(n_actions, activation='linear')\n","])\n","optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n","loss_fn = keras.losses.mean_squared_error"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10000 [00:00<?, ?it/s]\n"]},{"ename":"AttributeError","evalue":"'int' object has no attribute 'reshape'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\fayss\\Desktop\\IA-ID\\reinforcement_learning\\RL-project\\dqn-frozenlake.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fayss/Desktop/IA-ID/reinforcement_learning/RL-project/dqn-frozenlake.ipynb#X32sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m episode_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fayss/Desktop/IA-ID/reinforcement_learning/RL-project/dqn-frozenlake.ipynb#X32sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Store in replay buffer\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fayss/Desktop/IA-ID/reinforcement_learning/RL-project/dqn-frozenlake.ipynb#X32sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m target \u001b[39m=\u001b[39m reward \u001b[39mif\u001b[39;00m done \u001b[39melse\u001b[39;00m reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(model\u001b[39m.\u001b[39mpredict(next_state\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fayss/Desktop/IA-ID/reinforcement_learning/RL-project/dqn-frozenlake.ipynb#X32sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m target_f \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(state\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fayss/Desktop/IA-ID/reinforcement_learning/RL-project/dqn-frozenlake.ipynb#X32sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n","\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'reshape'"]}],"source":["# Metrics\n","total_rewards = []\n","total_steps = []\n","success_rate = []\n","\n","# Function to run episodes and train the agent\n","for episode in tqdm(range(n_episodes)):\n","    state = env.reset()\n","    done = False\n","    episode_reward = 0\n","    episode_steps = 0\n","\n","    while not done:\n","        # Epsilon-greedy action selection\n","        epsilon = max(1 - episode * epsilon_decay / (n_episodes - 1), 0.01)\n","        if np.random.rand() < epsilon:\n","            action = env.action_space.sample()\n","        else:\n","            q_values = model.predict(state.reshape(1, -1))\n","            action = np.argmax(q_values[0])\n","\n","        # Take action\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Modify reward for the hole case if needed\n","        if negative_reward_enabled:\n","            if done and reward == 0:  # Optionally penalize the agent for falling in a hole\n","                reward = -1\n","\n","        episode_reward += reward\n","        episode_steps += 1\n","\n","        # Store in replay buffer\n","        target = reward if done else reward + gamma * np.max(model.predict(next_state.reshape(1, -1))[0])\n","        target_f = model.predict(state.reshape(1, -1))\n","        target_f[0][action] = target\n","        model.fit(state.reshape(1, -1), target_f, epochs=1, verbose=0)\n","\n","        state = next_state\n","\n","    total_rewards.append(episode_reward)\n","    total_steps.append(episode_steps)\n","    success_rate.append(int(episode_reward > 0))\n","\n","    # Print metrics every 100 episodes\n","    if (episode + 1) % 100 == 0:\n","        average_reward = sum(total_rewards[-100:]) / 100\n","        average_steps = sum(total_steps[-100:]) / 100\n","        success_percentage = sum(success_rate[-100:]) / 100\n","        print(f\"Episode: {episode + 1}, Avg Reward: {average_reward}, Avg Steps: {average_steps}, Success Rate: {success_percentage}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Post training metrics\n","overall_average_reward = np.mean(total_rewards)\n","overall_average_steps = np.mean(total_steps)\n","overall_success_rate = np.mean(success_rate)\n","\n","print('----------------------------------------------------------')\n","print(\"Overall Average reward:\", overall_average_reward)\n","print(\"Overall Average number of steps:\", overall_average_steps)\n","print(\"Success rate (%):\", overall_success_rate * 100)\n","print('----------------------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting metrics\n","fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n","axs[0].plot(total_rewards, 'tab:green')\n","axs[0].set_title('Reward per Episode')\n","axs[1].plot(total_steps, 'tab:purple')\n","axs[1].set_title('Steps per Episode')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
