{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gym\nimport numpy as np\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-11-03T10:30:23.330667Z","iopub.execute_input":"2023-11-03T10:30:23.331121Z","iopub.status.idle":"2023-11-03T10:30:23.613095Z","shell.execute_reply.started":"2023-11-03T10:30:23.331024Z","shell.execute_reply":"2023-11-03T10:30:23.612082Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"env = gym.make('FrozenLake-v0', is_slippery=False)","metadata":{"execution":{"iopub.status.busy":"2023-11-03T10:30:23.614362Z","iopub.execute_input":"2023-11-03T10:30:23.614648Z","iopub.status.idle":"2023-11-03T10:30:24.287865Z","shell.execute_reply.started":"2023-11-03T10:30:23.614620Z","shell.execute_reply":"2023-11-03T10:30:24.287046Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-11-03T10:30:24.289947Z","iopub.execute_input":"2023-11-03T10:30:24.290328Z","iopub.status.idle":"2023-11-03T10:30:29.783865Z","shell.execute_reply.started":"2023-11-03T10:30:24.290289Z","shell.execute_reply":"2023-11-03T10:30:29.782912Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"obs_shape = 1\nn_actions = env.action_space.n\nmodel = keras.models.Sequential([\n                                 keras.layers.Dense(32, activation='relu', input_shape=[obs_shape]),\n                                 keras.layers.Dense(64, activation='relu'),\n                                 keras.layers.Dense(64, activation='relu'),\n                                 keras.layers.Dense(n_actions)\n])","metadata":{"execution":{"iopub.status.busy":"2023-11-03T10:30:29.785466Z","iopub.execute_input":"2023-11-03T10:30:29.785817Z","iopub.status.idle":"2023-11-03T10:30:32.123280Z","shell.execute_reply.started":"2023-11-03T10:30:29.785784Z","shell.execute_reply":"2023-11-03T10:30:32.122497Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def epsilon_greedy(state, epsilon):\n    if np.random.binomial(1, epsilon):\n        return np.random.randint(n_actions)\n    else:\n        return np.argmax(model.predict([state]))","metadata":{"execution":{"iopub.status.busy":"2023-11-03T10:30:32.124362Z","iopub.execute_input":"2023-11-03T10:30:32.124650Z","iopub.status.idle":"2023-11-03T10:30:32.129226Z","shell.execute_reply.started":"2023-11-03T10:30:32.124623Z","shell.execute_reply":"2023-11-03T10:30:32.128224Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"n_episodes = 10000\nreplay_buffer = []\nbatch_size = 32\ngamma = 0.98\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nloss_fn = keras.losses.mean_squared_error\n\ngoal_reached = []\n\nfor episode in tqdm(range(n_episodes)):\n    env.reset()\n    done = False\n    state = (0-7.5)/4.61\n    while not done:\n        #I CHOOSE A LINEAR DECAY OF EPSILON\n        epsilon = (-0.09/(n_episodes-1))*episode + 1\n        #PLAY THE GAME USING EPSILON-GREEDY WITH CURRENT POLICY UNTIL IT'S DONE\n        action = epsilon_greedy(state, epsilon)\n        next_state, reward, done, info = env.step(action)\n        #WE RECORD IF THE EPISODED ENDED ON THE GOAL CASE OR NOT\n        if done:\n            if next_state==15:\n                goal_reached.append(1)\n            else:\n                goal_reached.append(0)\n        #WE STORE THAT DATA INTO A BUFFER TO REPLAY FROM LATER AND TRAIN OUR MODEL\n        #IN THE BUFFER WE STORE THE TRAJECTORIES\n        replay_buffer.append((state, action, reward, (next_state-7.5)/4.61, done))\n        #THE NEXT_STATE BECOMES THE STATE FROM WHICH WE MOVE\n        state = (next_state-7.5)/4.61\n  \n    #AFTER PLAYING FOR A WHILE WE HAVE ENOUGH TRAJECTORIES WE CAN START TRAINING\n    if episode>32:\n        #WE SAMPLE batch_size TRAJECTORIES\n        indices = np.random.randint(len(replay_buffer), size=batch_size)\n        batch = [replay_buffer[index] for index in indices] \n        states, actions, rewards, next_states, dones = [np.array([trajectory[i] for trajectory in batch]) for i in range(5)]\n        #WE HAVE TO COMPUTE THE TARGET VALUES TO WHICH WE TRY TO APPROACH\n        target_Q_values = rewards + (1-dones)*gamma*np.max(model.predict(next_states), axis=1)\n        #TIME TO COMPUTE THE GRADIENTS\n        mask = tf.one_hot(actions, n_actions)\n        with tf.GradientTape() as tape:\n            #WE COMPUTE THE ACTUAL Q_VALUES\n            Q_values = model(states[:,np.newaxis])\n            Q_values = tf.reduce_sum(Q_values * mask, axis=1, keepdims=True)\n            #WE COMPUTE THE LOSS BETWEEN OUR ACTUAL Q_VALUES AND THE TARGET VALUES\n            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n        #WE COMPUTE THE GRADIENTS OF THE LOSS WITH RESPECT TO THE MODEL'S VARIABLES\n        grads = tape.gradient(loss, model.trainable_variables)\n        #WE APPLY THE GRADIENTS TO THE MODEL'S VARIABLES USING THE OPTIMIZER\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))","metadata":{"execution":{"iopub.status.busy":"2023-11-03T10:30:32.130425Z","iopub.execute_input":"2023-11-03T10:30:32.130772Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 91%|█████████ | 9079/10000 [07:51<00:59, 15.53it/s]","output_type":"stream"}]},{"cell_type":"code","source":"plt.plot(np.arange(n_episodes), goal_reached)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nprint(\"--------- Initial State ---------\")\nenv.render()\ndone = False\nstate = 0\nwhile not done:\n    print(\"--------- State \"+str(state)+\" ---------\")\n    action = np.argmax(model.predict([(state-7.5)/4.61]))\n    next_state, reward, done, info = env.step(action)\n    env.render()\n    state = next_state","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TRY FIXED Q_VALUES","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = keras.models.Sequential([\n                                 keras.layers.Dense(64, activation='relu', input_shape=[obs_shape]),\n                                 keras.layers.Dense(64, activation='relu'),\n                                 keras.layers.Dense(n_actions)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = keras.models.clone_model(model)\ntarget.set_weights(model.get_weights())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_episodes = 10000\nreplay_buffer = []\nbatch_size = 32\ngamma = 0.98\noptimizer = keras.optimizers.Adam(learning_rate=0.01)\nloss_fn = keras.losses.mean_squared_error\n\ngoal_reached = []\n\nfor episode in tqdm(range(n_episodes)):\n    env.reset()\n    done = False\n    state = (0-7.5)/4.61\n    while not done:\n        #I CHOOSE A LINEAR DECAY OF EPSILON\n        epsilon = (-0.09/(n_episodes-1))*episode + 1\n        #PLAY THE GAME USING EPSILON-GREEDY WITH CURRENT POLICY UNTIL IT'S DONE\n        action = epsilon_greedy(state, epsilon)\n        next_state, reward, done, info = env.step(action)\n        #WE RECORD IF THE EPISODED ENDED ON THE GOAL CASE OR NOT\n        if done:\n            if next_state==15:\n                goal_reached.append(1)\n            else:\n                goal_reached.append(0)\n        #WE STORE THAT DATA INTO A BUFFER TO REPLAY FROM LATER AND TRAIN OUR MODEL\n        #IN THE BUFFER WE STORE THE TRAJECTORIES\n        replay_buffer.append((state, action, reward, (next_state-7.5)/4.61, done))\n        #THE NEXT_STATE BECOMES THE STATE FROM WHICH WE MOVE\n        state = (next_state-7.5)/4.61\n  \n    #AFTER PLAYING FOR A WHILE WE HAVE ENOUGH TRAJECTORIES WE CAN START TRAINING\n    if episode>32:\n        #WE SAMPLE batch_size TRAJECTORIES\n        indices = np.random.randint(len(replay_buffer), size=batch_size)\n        batch = [replay_buffer[index] for index in indices] \n        states, actions, rewards, next_states, dones = [np.array([trajectory[i] for trajectory in batch]) for i in range(5)]\n        #WE HAVE TO COMPUTE THE TARGET VALUES TO WHICH WE TRY TO APPROACH\n        target_Q_values = rewards + (1-dones)*gamma*np.max(target.predict(next_states), axis=1)\n        #TIME TO COMPUTE THE GRADIENTS\n        mask = tf.one_hot(actions, n_actions)\n        with tf.GradientTape() as tape:\n            #WE COMPUTE THE ACTUAL Q_VALUES\n            Q_values = model(states[:,np.newaxis])\n            Q_values = tf.reduce_sum(Q_values * mask, axis=1, keepdims=True)\n            #WE COMPUTE THE LOSS BETWEEN OUR ACTUAL Q_VALUES AND THE TARGET VALUES\n            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n        #WE COMPUTE THE GRADIENTS OF THE LOSS WITH RESPECT TO THE MODEL'S VARIABLES\n        grads = tape.gradient(loss, model.trainable_variables)\n        #WE APPLY THE GRADIENTS TO THE MODEL'S VARIABLES USING THE OPTIMIZER\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    if episode % 64 == 0:\n        target.set_weights(model.get_weights())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.arange(n_episodes), goal_reached)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nprint(\"--------- Initial State ---------\")\nenv.render()\ndone = False\nstate = 0\nwhile not done:\n    print(\"--------- State \"+str(state)+\" ---------\")\n    action = np.argmax(model.predict([(state-7.5)/4.61]))\n    next_state, reward, done, info = env.step(action)\n    env.render()\n    state = next_state","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"obs_shape = 1\nn_actions = env.action_space.n\nmodel2 = keras.models.Sequential([\n                                 keras.layers.Dense(32, activation='relu', input_shape=[obs_shape]),\n                                 keras.layers.Dense(n_actions)\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target2 = keras.models.clone_model(model2)\ntarget2.set_weights(model2.get_weights())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epsilon_greedy2(state, epsilon):\n    if np.random.binomial(1, epsilon):\n        return np.random.randint(n_actions)\n    else:\n        return np.argmax(model2.predict([state]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_episodes = 1000000\nreplay_buffer = []\nbatch_size = 32\ngamma = 0.999\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\nloss_fn = keras.losses.mean_squared_error\n\ngoal_reached = []\n\nfor episode in tqdm(range(n_episodes)):\n    env.reset()\n    done = False\n    state = (0-7.5)/4.61\n    while not done:\n        #I CHOOSE A LINEAR DECAY OF EPSILON\n        epsilon = (-0.9/n_episodes)*episode +1\n        #PLAY THE GAME USING EPSILON-GREEDY WITH CURRENT POLICY UNTIL IT'S DONE\n        action = epsilon_greedy2(state, epsilon)\n        next_state, reward, done, info = env.step(action)\n        #WE RECORD IF THE EPISODED ENDED ON THE GOAL CASE OR NOT\n        if done:\n            if next_state==15:\n                goal_reached.append(1)\n            else:\n                goal_reached.append(0)\n        #WE STORE THAT DATA INTO A BUFFER TO REPLAY FROM LATER AND TRAIN OUR MODEL\n        #IN THE BUFFER WE STORE THE TRAJECTORIES\n        replay_buffer.append((state, action, reward, (next_state-7.5)/4.61, done))\n        #THE NEXT_STATE BECOMES THE STATE FROM WHICH WE MOVE\n        state = (next_state-7.5)/4.61\n  \n    #AFTER PLAYING FOR A WHILE WE HAVE ENOUGH TRAJECTORIES WE CAN START TRAINING\n    if episode>512:\n        #WE SAMPLE batch_size TRAJECTORIES\n        indices = np.random.randint(len(replay_buffer), size=batch_size)\n        batch = [replay_buffer[index] for index in indices] \n        states, actions, rewards, next_states, dones = [np.array([trajectory[i] for trajectory in batch]) for i in range(5)]\n        #WE HAVE TO COMPUTE THE TARGET VALUES TO WHICH WE TRY TO APPROACH\n        target_Q_values = rewards + (1-dones)*gamma*np.max(target2.predict(next_states), axis=1)\n        #TIME TO COMPUTE THE GRADIENTS\n        mask = tf.one_hot(actions, n_actions)\n        with tf.GradientTape() as tape:\n            #WE COMPUTE THE ACTUAL Q_VALUES\n            Q_values = model2(states[:,np.newaxis])\n            Q_values = tf.reduce_sum(Q_values * mask, axis=1, keepdims=True)\n            #WE COMPUTE THE LOSS BETWEEN OUR ACTUAL Q_VALUES AND THE TARGET VALUES\n            loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n        #WE COMPUTE THE GRADIENTS OF THE LOSS WITH RESPECT TO THE MODEL'S VARIABLES\n        grads = tape.gradient(loss, model2.trainable_variables)\n        #WE APPLY THE GRADIENTS TO THE MODEL'S VARIABLES USING THE OPTIMIZER\n        optimizer.apply_gradients(zip(grads, model2.trainable_variables))\n    if (episode>512) and (episode%256) == 0:\n        target2.set_weights(model2.get_weights())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.arange(n_episodes), goal_reached)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.reset()\nprint(\"--------- Initial State ---------\")\nenv.render()\ndone = False\nstate = 0\nwhile not done:\n    print(\"--------- State \"+str(state)+\" ---------\")\n    action = np.argmax(model2.predict([(state-7.5)/4.61]))\n    next_state, reward, done, info = env.step(action)\n    env.render()\n    state = next_state","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.arange(49000,50001), goal_reached[48999:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}